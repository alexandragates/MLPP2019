{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "\n",
    "- Due: 11:59pm, April 26, 2019\n",
    "\n",
    "In this project, you will work on sentiment classification with a logistic regression classifier in Python 3.  Using a large movie review dataset (http://ai.stanford.edu/~amaas/data/sentiment/), you will classify movie reviews into two categories, POSITIVE or NEGATIVE. \n",
    "\n",
    "You are provided with a training set (TRAIN), a development set (DEV), and a test set (TEST). Your classifier will be trained on TRAIN, evaluated and tuned on DEV, and tested on TEST. \n",
    "\n",
    "Using the PyTorch library, you will build the logistic regression classifier with bag of words features.  Some code has been provide  to help get you started.\n",
    "\n",
    "You need to fill in the missing code, run all cells, and submit this notebook along with a PDF with a writeup on your model tuning results and  solutions to the other problems in Homework 1.\n",
    "\n",
    "Credits: This assignment and notebook was originally created by Zewei Chu (zeweichu@uchicago.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import os, math\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "#UnicodeDecodeError\n",
    "\n",
    "# Feel free to define your own word_tokenizer instead of this naive \n",
    "# implementation. You may also use word_tokenize from nltk library \n",
    "# (from nltk import word_tokenize), which works better but slower. \n",
    "def word_tokenize(s):\n",
    "    return s.split()\n",
    "\n",
    "# set the random seeds so the experiments can be replicated exactly\n",
    "seed = 30255\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Global class labels.\n",
    "POS_LABEL = 'pos'\n",
    "NEG_LABEL = 'neg'     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file):\n",
    "    data = []\n",
    "    with open(data_file,'r', encoding=sys.getdefaultencoding()) as fin:\n",
    "        for line in fin:\n",
    "            label, content = line.split(\",\", 1)\n",
    "            data.append((content.lower(), label))\n",
    "    return data\n",
    "\n",
    "data_dir = \"large_movie_review_dataset\"\n",
    "train_data = load_data(os.path.join(data_dir, \"train.txt\"))\n",
    "dev_data = load_data(os.path.join(data_dir, \"dev.txt\"))\n",
    "test_data = load_data(os.path.join(data_dir, \"test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of TRAIN data 25000\n",
      "number of DEV data 5000\n",
      "number of TEST data 20000\n"
     ]
    }
   ],
   "source": [
    "print(\"number of TRAIN data\", len(train_data))\n",
    "print(\"number of DEV data\", len(dev_data))\n",
    "print(\"number of TEST data\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined a generic model class as below. The model has 2 functions, train and classify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "class Model:\n",
    "    def __init__(self, data):\n",
    "        # Vocabulary is a set that stores every word seen in the \n",
    "        # training data\n",
    "        self.vocab = Counter([word for content, label in data \n",
    "                              for word in word_tokenize(content)]\n",
    "                            ).most_common(VOCAB_SIZE-1)\n",
    "        # word to index mapping\n",
    "        self.word_to_idx = {k[0]: v+1 for v, k in \n",
    "                            enumerate(self.vocab)}\n",
    "        # all the unknown words will be mapped to index 0\n",
    "        self.word_to_idx[\"UNK\"] = 0 \n",
    "        self.idx_to_word = {v:k for k, v in self.word_to_idx.items()}\n",
    "        self.label_to_idx = {POS_LABEL: 0, NEG_LABEL: 1}\n",
    "        self.idx_to_label = [POS_LABEL, NEG_LABEL]\n",
    "        self.vocab = set(self.word_to_idx.keys())\n",
    "        \n",
    "    def train_model(self, data):\n",
    "        '''\n",
    "        Train the model with the provided training data\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def classify(self, data):\n",
    "        '''\n",
    "        Classify the documents with the model\n",
    "        '''\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Bag of Words\n",
    "\n",
    "(65 points)\n",
    "\n",
    "You will implement logistic regression with bag of words features. The code template is written with PyTorch. Reading the first two sections of the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html) will give you enough knowledge to code a logistic regression model with PyTorch. \n",
    "\n",
    "(When used for deep learning PyTorch code is usually run on GPUs (via the CUDA system).  In this homework, however, we'll use regular CPUs.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(tud.Dataset):\n",
    "    '''\n",
    "    PyTorch provides a common dataset interface. \n",
    "    See https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    The dataset encodes documents into indices. \n",
    "    With the PyTorch dataloader, you can easily get batched data for \n",
    "    training and evaluation. \n",
    "    '''\n",
    "    def __init__(self, word_to_idx, data):\n",
    "        \n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.label_to_idx = {POS_LABEL: 0, NEG_LABEL: 1}\n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = np.zeros(self.vocab_size)\n",
    "        item = torch.from_numpy(item)\n",
    "        # in training or tuning, we use both the document (review)\n",
    "        # and its corresponding label\n",
    "        if len(self.data[idx]) == 2: \n",
    "            for word in word_tokenize(self.data[idx][0]):\n",
    "                item[self.word_to_idx.get(word, 0)] += 1\n",
    "            label = self.label_to_idx[self.data[idx][1]]\n",
    "            return item, label\n",
    "        else: # in testing, we only use the document without label\n",
    "            for word in word_tokenize(self.data[idx]):\n",
    "                item[self.word_to_idx.get(word, 0)] += 1\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_model = None\n",
    "class BoWLRClassifier(nn.Module, Model):\n",
    "    '''\n",
    "    Define your logistic regression model with bag of words features.\n",
    "    '''\n",
    "    def __init__(self, data, loss=\"Cross\", optimizer=\"Adam\", learning_rate=1e-3):\n",
    "        nn.Module.__init__(self)\n",
    "        Model.__init__(self, data)\n",
    "        \n",
    "        '''\n",
    "        In this model initialization phase, write code to do the \n",
    "        following: \n",
    "        1. Define a linear layer to transform bag of words features \n",
    "           into 2 classes. \n",
    "        2. Define the loss function; use cross entropy loss (see\n",
    "            https://pytorch.org/docs/stable/nn.html?highlight=crossen#torch.nn.CrossEntropyLoss)\n",
    "        3. Define an optimizer for the model; choose the Adam optimizer,\n",
    "           which uses a version of the stochastic gradient descent \n",
    "           algorithm. (See https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.Adam)\n",
    "        '''\n",
    "        # linear layer\n",
    "        self.linear = nn.Linear(VOCAB_SIZE, 2)\n",
    "        \n",
    "        # define loss function\n",
    "        if loss == \"Cross\":\n",
    "            self.loss_function = nn.CrossEntropyLoss()\n",
    "        elif loss == \"NLL\":\n",
    "            self.loss_function = nn.NLLLoss()\n",
    "        \n",
    "        # define optimizer\n",
    "        if optimizer == \"Adam\":\n",
    "            self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        elif optimizer == \"SGD\":\n",
    "            self.optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, bow):\n",
    "        '''\n",
    "        Run the linear layer in the model for a single bag of words vector. \n",
    "        '''\n",
    "        return F.log_softmax(self.linear(bow), dim=1)\n",
    "    \n",
    "    def train_epoch(self, train_data):\n",
    "        '''\n",
    "        Train the model for one epoch with the training data\n",
    "        When training a model, you repeat the following procedure:\n",
    "        1. Get one batch of features and labels\n",
    "        2. Make a forward pass with the features to get predictions\n",
    "        3. Calculate the loss with the predictions and target labels\n",
    "        4. Run a backward pass from the loss function to get the gradients\n",
    "        5. Apply the optimizer step to update the model paramters\n",
    "        \n",
    "        For (1) you will have to understand how the PyTorch dataloader\n",
    "        functions.\n",
    "        '''\n",
    "        indexed_data = TextClassificationDataset(self.word_to_idx, train_data)\n",
    "        #print(indexed_data)\n",
    "        #trainloader = tud.DataLoader(indexed_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        \n",
    "        for feature, label in indexed_data:\n",
    "            log_probs = self.forward(feature.float().view(1,-1))\n",
    "            loss = self.loss_function(log_probs, torch.LongTensor([label]))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def classify(self, docs):\n",
    "        '''\n",
    "        This function classifies documents into their categories. \n",
    "        docs are documents without labels.\n",
    "        '''\n",
    "        log_probs = self.forward(docs.view(1, -1).float())\n",
    "        max_value, max_index = torch.max(log_probs, 1)\n",
    "        return max_value.item(), max_index.item()\n",
    "                \n",
    "    def evaluate_classifier_accuracy(self, data):\n",
    "        '''\n",
    "        This function evaluates the data with the current model. \n",
    "        data contains both documents and labels. \n",
    "        It calls classify() to make predictions, \n",
    "        and compares with the correct labels to return \n",
    "        the model accuracy on \"data\". \n",
    "        '''\n",
    "        num_right = 0\n",
    "        total = 0\n",
    "        correct_indexes = []\n",
    "        \n",
    "        for feature, index in data:\n",
    "            total += 1\n",
    "            pred_value, pred_index = self.classify(feature)\n",
    "            if pred_index == index:\n",
    "                num_right += 1\n",
    "                correct_indexes.append(index)\n",
    "\n",
    "        return num_right/total\n",
    "    \n",
    "    def train_model(self, train_data, dev_data, epochs):\n",
    "        \"\"\"\n",
    "        This function processes the entire training set for multiple epochs.\n",
    "        After each training epoch, evaluate your model on the DEV set. \n",
    "        Save the best performing model on the DEV set to best_model\n",
    "        \"\"\" \n",
    "        best_acc = 0\n",
    "        save_list = []\n",
    "        for epoch in range(epochs):\n",
    "            print(epoch)\n",
    "            \n",
    "            self.train_epoch(train_data)\n",
    "            print(\"trained\")\n",
    "        \n",
    "            indexed_data = TextClassificationDataset(self.word_to_idx, dev_data)\n",
    "            print(\"data indexed\")\n",
    "            \n",
    "            mod_acc = self.evaluate_classifier_accuracy(indexed_data)\n",
    "            print(\"accurracy calced\")\n",
    "            \n",
    "            if mod_acc > best_acc:\n",
    "                best_acc = mod_acc\n",
    "                print(best_acc)\n",
    "                save_list.append((epoch, best_acc*100)) \n",
    "                \n",
    "                best_mod = copy.deepcopy(self)\n",
    "            \n",
    "        return best_mod, best_acc\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n",
      "0.8134\n",
      "1\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n",
      "0.8196\n",
      "2\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n"
     ]
    }
   ],
   "source": [
    "lr_model = BoWLRClassifier(train_data, \"Cross\", \"Adam\", 1e-3)\n",
    "best_model, best_acc = lr_model.train_model(train_data, dev_data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the model\n",
    "\n",
    "(25 points)\n",
    "\n",
    "Now tune your model, by experimenting with\n",
    "\n",
    "- another optimizer\n",
    "- changing the learning rate\n",
    "- changing the number of epochs to train\n",
    "- adding regularization into your optimzer.\n",
    "\n",
    "Finally evaluate your tuned model on the TEST set.\n",
    "\n",
    "Report your results in a writeup, and submit that as a\n",
    "separate PDF file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n",
      "0.7804\n",
      "1\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n",
      "2\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n",
      "0.827\n"
     ]
    }
   ],
   "source": [
    "lr_model = BoWLRClassifier(train_data, \"Cross\", \"SGD\", 1e-3)\n",
    "best_model_2, best_acc_2 = lr_model.train_model(train_data, dev_data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n",
      "0.8162\n",
      "1\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n",
      "0.8274\n",
      "2\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n"
     ]
    }
   ],
   "source": [
    "lr_model = BoWLRClassifier(train_data, \"Cross\", \"Adam\", 0.01)\n",
    "best_model_3, best_acc_3 = lr_model.train_model(train_data, dev_data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change Number of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n",
      "0.8166\n",
      "1\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n",
      "0.8268\n",
      "2\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n",
      "3\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n",
      "4\n",
      "trained\n",
      "data indexed\n",
      "accurracy calced\n",
      "0.831\n"
     ]
    }
   ],
   "source": [
    "lr_model = BoWLRClassifier(train_data, \"Cross\", \"Adam\", 1e-3)\n",
    "best_model_4, best_acc_4 = lr_model.train_model(train_data, dev_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = [best_acc, best_acc_2, best_acc_3, best_acc_4]\n",
    "mods = [best_model, best_model_2, best_model_3, best_model_4]\n",
    "high = np.argmax(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accurracy of the chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "16537\n",
      "20000\n",
      "0.82685\n"
     ]
    }
   ],
   "source": [
    "chosen_model = mods[high]\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "features = [i[0] for i in test_data]\n",
    "indices = [i[1] for i in test_data]\n",
    "\n",
    "indexed_features = TextClassificationDataset(chosen_model.word_to_idx, features)\n",
    "for info in indexed_features:\n",
    "    val, idx = chosen_model.classify(info)\n",
    "    classification = chosen_model.idx_to_label[idx]\n",
    "    if classification == indices[total]:\n",
    "        correct += 1\n",
    "    \n",
    "    total += 1\n",
    "    \n",
    "    if total % 1000  == 0:\n",
    "        print(total)\n",
    "\n",
    "print(correct)\n",
    "print(total)\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature analysis\n",
    "\n",
    "(10 points)\n",
    "\n",
    "Write code for each of the following, and include an analysis of the results in your writup.\n",
    "\n",
    "\n",
    "- Identify the top 10 features with the maximum positive weights for POSITIVE category. \n",
    "\n",
    "- Identify the top 10 features with the maximum negative weights for POSITIVE category. \n",
    "\n",
    "- Identify the top 10 features with the maximum positive weights for NEGATIVE category. \n",
    "\n",
    "- Identify the top 10 features with the maximum negative weights for NEGATIVE category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = chosen_model.state_dict()\n",
    "pos = mod['linear.weight'][0]\n",
    "neg = mod['linear.weight'][1]\n",
    "\n",
    "def get_words(idx, words_to_idx):\n",
    "    top_words = []\n",
    "    for word, val in words_to_idx.items():\n",
    "        if val in idx:\n",
    "            top_words.append(word)\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wonderfully',\n",
       " 'noir',\n",
       " 'delightful',\n",
       " 'excellent,',\n",
       " 'lonely',\n",
       " 'perfect.',\n",
       " 'perfect,',\n",
       " 'complaint',\n",
       " '8/10',\n",
       " 'can.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pos_wt_pos_cat_idx = np.argsort(pos)[-10:]\n",
    "words = get_words(top_pos_wt_pos_cat_idx, chosen_model.word_to_idx)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['redeeming',\n",
       " 'insult',\n",
       " 'disappointment',\n",
       " 'unfunny',\n",
       " 'horrible.',\n",
       " 'dull,',\n",
       " 'wasting',\n",
       " 'garbage.',\n",
       " 'unconvincing',\n",
       " 'pathetic.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_neg_wt_pos_cat_idx = np.argsort(pos)[:10]\n",
    "words = get_words(top_neg_wt_pos_cat_idx, chosen_model.word_to_idx)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['redeeming',\n",
       " 'insult',\n",
       " 'disappointment',\n",
       " 'unfunny',\n",
       " 'horrible.',\n",
       " 'dull,',\n",
       " 'wasting',\n",
       " 'garbage.',\n",
       " 'unconvincing',\n",
       " 'pathetic.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pos_wt_neg_cat_idx = np.argsort(neg)[-10:]\n",
    "words = get_words(top_pos_wt_neg_cat_idx, chosen_model.word_to_idx)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wonderfully',\n",
       " 'noir',\n",
       " 'delightful',\n",
       " 'excellent,',\n",
       " 'lonely',\n",
       " 'perfect.',\n",
       " 'perfect,',\n",
       " 'complaint',\n",
       " '8/10',\n",
       " 'can.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_neg_wt_neg_cat_idx = np.argsort(neg)[:10]\n",
    "words = get_words(top_neg_wt_neg_cat_idx, chosen_model.word_to_idx)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
